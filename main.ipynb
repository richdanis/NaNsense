{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "h0EPX9TBwTq6",
   "metadata": {
    "id": "h0EPX9TBwTq6"
   },
   "source": [
    "## **üõ†Ô∏è Tools You May Consider**  \n",
    "(*These are recommendations to help you get started. You are free to use alternative tools‚Äîjust document your choices clearly!*)  \n",
    "- **Database**: FAISS, ChromaDB, SQLite, Elasticsearch, Neo4j and etc.  \n",
    "- **Embedding Models**: Hugging Face Sentence-Transformers, OpenAI Embeddings  \n",
    "- **LLM for Generation**: OpenAI: gpt-4o-mini\n",
    "- **Others**: Langchain, GraphRAG, and etc.\n",
    "\n",
    "## **üìå Final Delivery**  \n",
    "Your final submission should include:  \n",
    "‚úÖ A well-documented **GitHub repository or notebook**  \n",
    "‚úÖ A clear **README** explaining your approach  \n",
    "‚úÖ A structured **retrieval and generation modules**  \n",
    "\n",
    "### **üî• Bonus Points For**  \n",
    "‚ú® Innovative retrieval techniques  \n",
    "‚ú® Well-organized, modular code  \n",
    "‚ú® Creative visualizations or user interfaces  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Ipz5In6W_NIx",
   "metadata": {
    "id": "Ipz5In6W_NIx"
   },
   "source": [
    "# 1. Set up working environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EQPuHKS4QQjn",
   "metadata": {
    "id": "EQPuHKS4QQjn"
   },
   "source": [
    "# 2. Knowledge Base Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "C1ZmEq85_XvG",
   "metadata": {
    "id": "C1ZmEq85_XvG"
   },
   "source": [
    "## 2.1 Load documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2GpLOcX8xoQ1",
   "metadata": {
    "id": "2GpLOcX8xoQ1"
   },
   "source": [
    "Once you are added access to this folder, it will appear at your google drive \"Shared drives\". Then you can mount your drive and as following, and access your data from \"/content/drive/Shared drives/Datathon/Data/hackathon_data/\". Enjoy the ride! :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "kUvCnCVnRf5c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kUvCnCVnRf5c",
    "outputId": "f920db58-f4b9-4813-c0e5-8f8520c7a7cc"
   },
   "outputs": [],
   "source": [
    "# Load the Drive and mount\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jQt4yk-y_LqU",
   "metadata": {
    "id": "jQt4yk-y_LqU"
   },
   "source": [
    "Load json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c70eec4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered: cabotcorp.com.json (kept 68/70 pages)\n",
      "Filtered: thedesignpeople.com.json (kept 69/70 pages)\n",
      "Filtered: stenograph.com.json (kept 70/70 pages)\n",
      "Filtered: cleaningguys.com.json (kept 70/70 pages)\n",
      "Filtered: fmssolutions.com.json (kept 69/70 pages)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from src.preprocessing import filter_json_file\n",
    "\n",
    "for filename in os.listdir(\"data/hackathon_data\")[:5]:\n",
    "    if filename.endswith(\".json\"):\n",
    "        filepath = os.path.join(\"data/hackathon_data\", filename)\n",
    "        filter_json_file(filepath, \"data/clean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LTUgoion_dLj",
   "metadata": {
    "id": "LTUgoion_dLj"
   },
   "source": [
    "## 2.2 Pre-process documents.\n",
    "\n",
    "Feel free to explore and pre-process the data. You may want to clean or segment the documents as you see fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "x_3mbRONy7M7",
   "metadata": {
    "id": "x_3mbRONy7M7"
   },
   "outputs": [],
   "source": [
    "def document_clean(docs):\n",
    "  \"\"\"\n",
    "  You may want to clean the dataset, add the code here.\n",
    "  \"\"\"\n",
    "  pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qrhr__MeK-hY",
   "metadata": {
    "id": "qrhr__MeK-hY"
   },
   "source": [
    "## 2.3 Document Indexing and Storage (Profiling)\n",
    "\n",
    "Feel free to choose different ways to indexing and storing the provided documents in a knowledge database.\n",
    "\n",
    "So that they can be retrieved in different ways according to your system design choices, such as search by keywords, vector representation, graph relation, and etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f566673",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "from langchain_community.document_loaders import JSONLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "import json\n",
    "\n",
    "def chunk_documents(documents, chunk_size=500, chunk_overlap=100):\n",
    "    \"\"\"\n",
    "    Split documents into chunks for better retrieval.\n",
    "    \n",
    "    Args:\n",
    "        documents: List of document dictionaries with content and metadata\n",
    "        chunk_size: Maximum size of chunks\n",
    "        chunk_overlap: Overlap between chunks\n",
    "    \n",
    "    Returns:\n",
    "        List of LangChain Document objects\n",
    "    \"\"\"\n",
    "    from langchain.schema import Document\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "    )\n",
    "    \n",
    "    chunked_docs = []\n",
    "    for doc in documents:\n",
    "        splits = text_splitter.split_text(doc[\"content\"])\n",
    "        for i, split in enumerate(splits):\n",
    "            chunked_docs.append(\n",
    "                Document(\n",
    "                    page_content=split,\n",
    "                    metadata={\n",
    "                        **doc[\"metadata\"],\n",
    "                        \"chunk_id\": i\n",
    "                    }\n",
    "                )\n",
    "            )\n",
    "    \n",
    "    return chunked_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d628c259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# go over the data/clean folder and chunk the documents\n",
    "documents = []\n",
    "for filename in os.listdir(\"data/clean\"):\n",
    "    if filename.endswith(\".json\"):\n",
    "        filepath = os.path.join(\"data/clean\", filename)\n",
    "        with open(filepath, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "            for url in data[\"text_by_page_url\"]:\n",
    "                documents.append({\"content\": data[\"text_by_page_url\"][url], \"metadata\": {\"source\": url}})\n",
    "\n",
    "documents = chunk_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba378dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = documents[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "woyxfDBeN_mG",
   "metadata": {
    "id": "woyxfDBeN_mG"
   },
   "source": [
    "# 3. Retrieval Augmented Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hSBIL9B-k1wE",
   "metadata": {
    "id": "hSBIL9B-k1wE"
   },
   "source": [
    "## 3.1 Load Knowledge Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19a5f6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Replace OpenAI embeddings with a local model\n",
    "def get_local_embeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "    \"\"\"\n",
    "    Create a local embedding model using HuggingFace models.\n",
    "    \n",
    "    Args:\n",
    "        model_name: Name of the HuggingFace embedding model\n",
    "    \n",
    "    Returns:\n",
    "        HuggingFaceEmbeddings model\n",
    "    \"\"\"\n",
    "    model_kwargs = {'device': 'cpu'}  # Use 'cuda' if you have a GPU\n",
    "    encode_kwargs = {'normalize_embeddings': True}\n",
    "    \n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=model_name,\n",
    "        model_kwargs=model_kwargs,\n",
    "        encode_kwargs=encode_kwargs\n",
    "    )\n",
    "    \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eac4ac39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector_db(documents, persist_directory=\"./chroma_db\"):\n",
    "    \"\"\"\n",
    "    Create and persist a vector database from documents.\n",
    "    \n",
    "    Args:\n",
    "        documents: List of LangChain Document objects\n",
    "        embedding_model_name: Name of the OpenAI embedding model to use\n",
    "        persist_directory: Directory to save the vector database\n",
    "    \n",
    "    Returns:\n",
    "        Chroma vector store\n",
    "    \"\"\"\n",
    "    # Initialize the embedding model\n",
    "    embeddings = get_local_embeddings()\n",
    "    \n",
    "    # Create and persist the vector store\n",
    "    vectordb = Chroma.from_documents(\n",
    "        documents=documents,\n",
    "        embedding=embeddings,\n",
    "        persist_directory=persist_directory,\n",
    "\n",
    "    )\n",
    "    \n",
    "    vectordb.persist()\n",
    "    print(f\"Vector database created with {len(documents)} chunks and saved to {persist_directory}\")\n",
    "    \n",
    "    return vectordb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c5f0c1fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector database created with 5 chunks and saved to ./chroma_db\n"
     ]
    }
   ],
   "source": [
    "vector_db = create_vector_db(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "X6jZ743wLayQ",
   "metadata": {
    "id": "X6jZ743wLayQ"
   },
   "source": [
    "## 3.2 Relevant Document Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xj5pH-FATv6S",
   "metadata": {
    "id": "xj5pH-FATv6S"
   },
   "source": [
    "Feel free to check and improve your retrieval performance as it affect the generation results significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6a647547",
   "metadata": {
    "id": "6a647547"
   },
   "outputs": [],
   "source": [
    "def retrieve_documents(query, vectordb, k=1):\n",
    "    \"\"\"\n",
    "    Retrieve relevant documents from the vector database based on the query.\n",
    "    \n",
    "    Args:\n",
    "        query: User query string\n",
    "        vectordb: Vector database to search\n",
    "        k: Number of documents to retrieve\n",
    "    \n",
    "    Returns:\n",
    "        List of retrieved documents\n",
    "    \"\"\"\n",
    "    retriever = vectordb.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": k})\n",
    "    docs = retriever.get_relevant_documents(query)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kYJCpsgHLoc-",
   "metadata": {
    "id": "kYJCpsgHLoc-"
   },
   "source": [
    "## 3.3 Response Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "-6xIzmDTn3I8",
   "metadata": {
    "id": "-6xIzmDTn3I8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31865/292980324.py:14: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs = retriever.get_relevant_documents(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What company is located in 29010 Commerce Center Dr., Valencia, 91355, California, US?\n",
      "Retrieved Documents: ['ABC Corporation is located at 29010 Commerce Center Dr., Valencia, 91355, California, US.']\n",
      "Generated Answer: I don't have enough information to answer this question.\n"
     ]
    }
   ],
   "source": [
    "from src.prompts import generate_answer, load_prompts\n",
    "\n",
    "query = \"What company is located in 29010 Commerce Center Dr., Valencia, 91355, California, US?\"\n",
    "retrieved_docs = retrieve_documents(query, vector_db)\n",
    "prompts = load_prompts()\n",
    "prompt_template = prompts[\"rag_default\"]\n",
    "response = generate_answer(query, retrieved_texts=retrieved_docs, prompt_template=prompt_template, model=\"gpt-4o\")\n",
    "\n",
    "print(\"Query:\", query)\n",
    "print(\"Retrieved Documents:\", [\"ABC Corporation is located at 29010 Commerce Center Dr., Valencia, 91355, California, US.\"])\n",
    "print(\"Generated Answer:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RK8CQELywSf1",
   "metadata": {
    "id": "RK8CQELywSf1"
   },
   "source": [
    "# 4. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfd94a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
