{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "h0EPX9TBwTq6",
   "metadata": {
    "id": "h0EPX9TBwTq6"
   },
   "source": [
    "## **ðŸš€ NaNsense**  \n",
    "- **Database**: ChromaDB\n",
    "- **Embedding Model**: sentence-transformers/all-MiniLM-L6-v2\n",
    "- **LLM for Generation**: gpt-4o\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EQPuHKS4QQjn",
   "metadata": {
    "id": "EQPuHKS4QQjn"
   },
   "source": [
    "# 1. Knowledge Base Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "C1ZmEq85_XvG",
   "metadata": {
    "id": "C1ZmEq85_XvG"
   },
   "source": [
    "## 1.1 Pre-process documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70eec4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "from src.preprocessing import filter_json_file\n",
    "\n",
    "folder_path = \"data/hackathon_data\"\n",
    "\n",
    "if not os.path.exists(\"data/clean\"):\n",
    "    os.makedirs(\"data/clean\")\n",
    "    for filename in tqdm(os.listdir(folder_path)):\n",
    "        if filename.endswith(\".json\"):\n",
    "            filepath = os.path.join(folder_path, filename)\n",
    "            filter_json_file(filepath, \"data/clean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qrhr__MeK-hY",
   "metadata": {
    "id": "qrhr__MeK-hY"
   },
   "source": [
    "## 2.3 Document Indexing and Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f566673",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def chunk_documents(documents, chunk_size=500, chunk_overlap=100):\n",
    "    \"\"\"\n",
    "    Split documents into chunks for better retrieval.\n",
    "    \"\"\"\n",
    "    from langchain.schema import Document\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "    )\n",
    "    \n",
    "    chunked_docs = []\n",
    "    for doc in tqdm(documents):\n",
    "        splits = text_splitter.split_text(doc[\"content\"])\n",
    "        for i, split in enumerate(splits):\n",
    "            chunked_docs.append(\n",
    "                Document(\n",
    "                    page_content=split,\n",
    "                    metadata={\n",
    "                        **doc[\"metadata\"],\n",
    "                        \"chunk_id\": i\n",
    "                    }\n",
    "                )\n",
    "            )\n",
    "    \n",
    "    return chunked_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bd5bd5",
   "metadata": {},
   "source": [
    "#### Option 1: Wihout Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d628c259",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13144/13144 [00:12<00:00, 1070.66it/s]\n"
     ]
    }
   ],
   "source": [
    "# go over the data/clean folder and chunk the documents\n",
    "documents = []\n",
    "for filename in tqdm(os.listdir(\"data/clean\")):\n",
    "    if filename.endswith(\".json\"):\n",
    "        filepath = os.path.join(\"data/clean\", filename)\n",
    "        with open(filepath, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "            for url in data[\"text_by_page_url\"]:\n",
    "                documents.append({\"content\": data[\"text_by_page_url\"][url], \"metadata\": {\"source\": url}})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2660ee",
   "metadata": {},
   "source": [
    "#### Option 2: With Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b59684",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.fuzzy_metadata import fuzzy_is_meta\n",
    "\n",
    "documents= fuzzy_is_meta(use_all_doc=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10d6852",
   "metadata": {},
   "source": [
    "#### Continue as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a872b54d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 258097/258097 [02:01<00:00, 2123.07it/s]\n"
     ]
    }
   ],
   "source": [
    "chunked_documents = chunk_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dccf678a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fuzzy searching: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3976253/3976253 [01:21<00:00, 49037.01it/s]\n"
     ]
    }
   ],
   "source": [
    "from src.keyword_retrieval import fuzzy_search\n",
    "\n",
    "keywords = fuzzy_search([\"France\", \"Cheese\", \"Wine\"], chunked_documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54d59b5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'https://www.thehenryrestaurant.com/locations/the-henry-west-hollywood/menus/dinner-menu/', 'chunk_id': 2}, page_content='fig, pumpkin seed, candied pecan, pecorino, mustard vinaigrette EntrÃ©es Wagyu Cheeseburger* 25 lettuce, tomato, pickle, charred onion, white cheddar, american cheese, henry sauce Scottish Salmon* 38 toasted quinoa, marcona almond pesto, crispy sweet potato, watercress, pomegranate glaze Filet Mignon* 56 horseradish gratin, roasted brussels sprout, wild mushroom, cipollini onion, burgundy sauce Add Lobster 24 Bolognese 29 garganelli pasta, truffle mushroom butter, herbed ricotta, garlic toast')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords[20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "woyxfDBeN_mG",
   "metadata": {
    "id": "woyxfDBeN_mG"
   },
   "source": [
    "# 3. Retrieval Augmented Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hSBIL9B-k1wE",
   "metadata": {
    "id": "hSBIL9B-k1wE"
   },
   "source": [
    "## 3.1 Load Knowledge Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19a5f6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "def get_local_embeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "    \"\"\"\n",
    "    Create a local embedding model using HuggingFace models.\n",
    "    \"\"\"\n",
    "    model_kwargs = {'device': 'cuda' if torch.cuda.is_available() else 'cpu'}  # Use 'cuda' if you have a GPU\n",
    "    encode_kwargs = {'normalize_embeddings': True}\n",
    "    \n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=model_name,\n",
    "        model_kwargs=model_kwargs,\n",
    "        encode_kwargs=encode_kwargs\n",
    "    )\n",
    "    \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eac4ac39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector_db(documents, persist_directory=\"./chroma_db\"):\n",
    "    \"\"\"\n",
    "    Create and persist a vector database from documents.\n",
    "    \"\"\"\n",
    "    embeddings = get_local_embeddings()\n",
    "    \n",
    "    vectordb = Chroma.from_documents(\n",
    "        documents=documents,\n",
    "        embedding=embeddings,\n",
    "        persist_directory=persist_directory,\n",
    "\n",
    "    )\n",
    "    \n",
    "    vectordb.persist()\n",
    "    print(f\"Vector database created with {len(documents)} chunks and saved to {persist_directory}\")\n",
    "    \n",
    "    return vectordb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f0c1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_db = create_vector_db(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "X6jZ743wLayQ",
   "metadata": {
    "id": "X6jZ743wLayQ"
   },
   "source": [
    "## 3.2 Relevant Document Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xj5pH-FATv6S",
   "metadata": {
    "id": "xj5pH-FATv6S"
   },
   "source": [
    "Feel free to check and improve your retrieval performance as it affect the generation results significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6a647547",
   "metadata": {
    "id": "6a647547"
   },
   "outputs": [],
   "source": [
    "def retrieve_documents(query, vectordb, k=1):\n",
    "    \"\"\"\n",
    "    Retrieve relevant documents from the vector database based on the query.\n",
    "    \"\"\"\n",
    "    retriever = vectordb.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": k})\n",
    "    docs = retriever.get_relevant_documents(query)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kYJCpsgHLoc-",
   "metadata": {
    "id": "kYJCpsgHLoc-"
   },
   "source": [
    "## 3.3 Response Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-6xIzmDTn3I8",
   "metadata": {
    "id": "-6xIzmDTn3I8"
   },
   "outputs": [],
   "source": [
    "from src.prompts import generate_answer, load_prompts\n",
    "\n",
    "query = \"What company is located in 29010 Commerce Center Dr., Valencia, 91355, California, US?\"\n",
    "retrieved_docs = retrieve_documents(query, vector_db)\n",
    "prompts = load_prompts()\n",
    "prompt_template = prompts[\"rag_default\"]\n",
    "response = generate_answer(query, retrieved_texts=retrieved_docs, prompt_template=prompt_template, model=\"gpt-4o\")\n",
    "\n",
    "print(\"Query:\", query)\n",
    "print(\"Retrieved Documents:\", [\"ABC Corporation is located at 29010 Commerce Center Dr., Valencia, 91355, California, US.\"])\n",
    "print(\"Generated Answer:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RK8CQELywSf1",
   "metadata": {
    "id": "RK8CQELywSf1"
   },
   "source": [
    "# 4. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfd94a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluate import evaluate_rag_system, save_evaluation_results\n",
    "\n",
    "benchmark_file = \"benchmark.json\"\n",
    "k = 3\n",
    "prompt_template_name = \"rag_default\"\n",
    "model = \"gpt-4o\"\n",
    "\n",
    "print(\"Starting evaluation...\")\n",
    "evaluation_results = evaluate_rag_system(\n",
    "    benchmark_file=benchmark_file,\n",
    "    k=k,\n",
    "    prompt_template_name=prompt_template_name,\n",
    "    model=model\n",
    ")\n",
    "\n",
    "print(\"\\nEvaluation Results:\")\n",
    "for metric, value in evaluation_results[\"metrics\"].items():\n",
    "    print(\n",
    "        f\"{metric}: {value:.2f}%\"\n",
    "        if \"percentage\" in metric\n",
    "        else f\"{metric}: {value}\"\n",
    "    )\n",
    "\n",
    "save_evaluation_results(evaluation_results, \"evaluation_results.json\")\n",
    "\n",
    "print(\"\\nExample Results:\")\n",
    "for i, result in enumerate(evaluation_results[\"results\"][:3]):  # Show first 3 examples\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Question: {result['question']}\")\n",
    "    print(f\"Reference: {result['reference_answer']}\")\n",
    "    print(f\"Prediction: {result['predicted_answer']}\")\n",
    "    print(f\"Exact Match: {'Yes' if result['exact_match'] == 1.0 else 'No'}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
