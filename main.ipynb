{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "h0EPX9TBwTq6",
   "metadata": {
    "id": "h0EPX9TBwTq6"
   },
   "source": [
    "## **üõ†Ô∏è Tools You May Consider**  \n",
    "(*These are recommendations to help you get started. You are free to use alternative tools‚Äîjust document your choices clearly!*)  \n",
    "- **Database**: FAISS, ChromaDB, SQLite, Elasticsearch, Neo4j and etc.  \n",
    "- **Embedding Models**: Hugging Face Sentence-Transformers, OpenAI Embeddings  \n",
    "- **LLM for Generation**: OpenAI: gpt-4o-mini\n",
    "- **Others**: Langchain, GraphRAG, and etc.\n",
    "\n",
    "## **üìå Final Delivery**  \n",
    "Your final submission should include:  \n",
    "‚úÖ A well-documented **GitHub repository or notebook**  \n",
    "‚úÖ A clear **README** explaining your approach  \n",
    "‚úÖ A structured **retrieval and generation modules**  \n",
    "\n",
    "### **üî• Bonus Points For**  \n",
    "‚ú® Innovative retrieval techniques  \n",
    "‚ú® Well-organized, modular code  \n",
    "‚ú® Creative visualizations or user interfaces  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Ipz5In6W_NIx",
   "metadata": {
    "id": "Ipz5In6W_NIx"
   },
   "source": [
    "# 1. Set up working environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a14a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/richdanis/NaNsense.git\n",
    "!cd NaNsense\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265edb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Drive and mount\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EQPuHKS4QQjn",
   "metadata": {
    "id": "EQPuHKS4QQjn"
   },
   "source": [
    "# 2. Knowledge Base Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "C1ZmEq85_XvG",
   "metadata": {
    "id": "C1ZmEq85_XvG"
   },
   "source": [
    "## 2.1 Load documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2GpLOcX8xoQ1",
   "metadata": {
    "id": "2GpLOcX8xoQ1"
   },
   "source": [
    "Once you are added access to this folder, it will appear at your google drive \"Shared drives\". Then you can mount your drive and as following, and access your data from \"/content/drive/Shared drives/Datathon/Data/hackathon_data/\". Enjoy the ride! :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fdc8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"/content/drive/Shared drives/Datathon/Data/hackathon_data/\"# Google drive path of the dataset\n",
    "folder_path = \"data/hackathon_data\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jQt4yk-y_LqU",
   "metadata": {
    "id": "jQt4yk-y_LqU"
   },
   "source": [
    "Load json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0443af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir data/clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70eec4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "from src.preprocessing import filter_json_file\n",
    "\n",
    "for filename in tqdm(os.listdir(folder_path)):\n",
    "    if filename.endswith(\".json\"):\n",
    "        filepath = os.path.join(folder_path, filename)\n",
    "        filter_json_file(filepath, \"data/clean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LTUgoion_dLj",
   "metadata": {
    "id": "LTUgoion_dLj"
   },
   "source": [
    "## 2.2 Pre-process documents.\n",
    "\n",
    "Feel free to explore and pre-process the data. You may want to clean or segment the documents as you see fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "x_3mbRONy7M7",
   "metadata": {
    "id": "x_3mbRONy7M7"
   },
   "outputs": [],
   "source": [
    "def document_clean(docs):\n",
    "  \"\"\"\n",
    "  You may want to clean the dataset, add the code here.\n",
    "  \"\"\"\n",
    "  pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qrhr__MeK-hY",
   "metadata": {
    "id": "qrhr__MeK-hY"
   },
   "source": [
    "## 2.3 Document Indexing and Storage (Profiling)\n",
    "\n",
    "Feel free to choose different ways to indexing and storing the provided documents in a knowledge database.\n",
    "\n",
    "So that they can be retrieved in different ways according to your system design choices, such as search by keywords, vector representation, graph relation, and etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f566673",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "from langchain_community.document_loaders import JSONLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def chunk_documents(documents, chunk_size=500, chunk_overlap=100):\n",
    "    \"\"\"\n",
    "    Split documents into chunks for better retrieval.\n",
    "    \n",
    "    Args:\n",
    "        documents: List of document dictionaries with content and metadata\n",
    "        chunk_size: Maximum size of chunks\n",
    "        chunk_overlap: Overlap between chunks\n",
    "    \n",
    "    Returns:\n",
    "        List of LangChain Document objects\n",
    "    \"\"\"\n",
    "    from langchain.schema import Document\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "    )\n",
    "    \n",
    "    chunked_docs = []\n",
    "    for doc in tqdm(documents):\n",
    "        splits = text_splitter.split_text(doc[\"content\"])\n",
    "        for i, split in enumerate(splits):\n",
    "            chunked_docs.append(\n",
    "                Document(\n",
    "                    page_content=split,\n",
    "                    metadata={\n",
    "                        **doc[\"metadata\"],\n",
    "                        \"chunk_id\": i\n",
    "                    }\n",
    "                )\n",
    "            )\n",
    "    \n",
    "    return chunked_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bd5bd5",
   "metadata": {},
   "source": [
    "#### Option 1: Wihout Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d628c259",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13144/13144 [00:12<00:00, 1070.66it/s]\n"
     ]
    }
   ],
   "source": [
    "# go over the data/clean folder and chunk the documents\n",
    "documents = []\n",
    "for filename in tqdm(os.listdir(\"data/clean\")):\n",
    "    if filename.endswith(\".json\"):\n",
    "        filepath = os.path.join(\"data/clean\", filename)\n",
    "        with open(filepath, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "            for url in data[\"text_by_page_url\"]:\n",
    "                documents.append({\"content\": data[\"text_by_page_url\"][url], \"metadata\": {\"source\": url}})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2660ee",
   "metadata": {},
   "source": [
    "#### Option 2: With Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b59684",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.fuzzy_metadata import fuzzy_is_meta\n",
    "\n",
    "documents= fuzzy_is_meta(use_all_doc=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10d6852",
   "metadata": {},
   "source": [
    "#### Continue as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a872b54d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 258097/258097 [02:01<00:00, 2123.07it/s]\n"
     ]
    }
   ],
   "source": [
    "chunked_documents = chunk_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dccf678a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fuzzy searching: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3976253/3976253 [01:21<00:00, 49037.01it/s]\n"
     ]
    }
   ],
   "source": [
    "from src.keyword_retrieval import fuzzy_search\n",
    "\n",
    "keywords = fuzzy_search([\"France\", \"Cheese\", \"Wine\"], chunked_documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54d59b5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'https://www.thehenryrestaurant.com/locations/the-henry-west-hollywood/menus/dinner-menu/', 'chunk_id': 2}, page_content='fig, pumpkin seed, candied pecan, pecorino, mustard vinaigrette Entr√©es Wagyu Cheeseburger* 25 lettuce, tomato, pickle, charred onion, white cheddar, american cheese, henry sauce Scottish Salmon* 38 toasted quinoa, marcona almond pesto, crispy sweet potato, watercress, pomegranate glaze Filet Mignon* 56 horseradish gratin, roasted brussels sprout, wild mushroom, cipollini onion, burgundy sauce Add Lobster 24 Bolognese 29 garganelli pasta, truffle mushroom butter, herbed ricotta, garlic toast')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords[20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "woyxfDBeN_mG",
   "metadata": {
    "id": "woyxfDBeN_mG"
   },
   "source": [
    "# 3. Retrieval Augmented Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa11e84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "hSBIL9B-k1wE",
   "metadata": {
    "id": "hSBIL9B-k1wE"
   },
   "source": [
    "## 3.1 Load Knowledge Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19a5f6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Replace OpenAI embeddings with a local model\n",
    "def get_local_embeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "    \"\"\"\n",
    "    Create a local embedding model using HuggingFace models.\n",
    "    \n",
    "    Args:\n",
    "        model_name: Name of the HuggingFace embedding model\n",
    "    \n",
    "    Returns:\n",
    "        HuggingFaceEmbeddings model\n",
    "    \"\"\"\n",
    "    model_kwargs = {'device': 'cuda' if torch.cuda.is_available() else 'cpu'}  # Use 'cuda' if you have a GPU\n",
    "    encode_kwargs = {'normalize_embeddings': True}\n",
    "    \n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=model_name,\n",
    "        model_kwargs=model_kwargs,\n",
    "        encode_kwargs=encode_kwargs\n",
    "    )\n",
    "    \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eac4ac39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector_db(documents, persist_directory=\"./chroma_db\"):\n",
    "    \"\"\"\n",
    "    Create and persist a vector database from documents.\n",
    "    \n",
    "    Args:\n",
    "        documents: List of LangChain Document objects\n",
    "        embedding_model_name: Name of the OpenAI embedding model to use\n",
    "        persist_directory: Directory to save the vector database\n",
    "    \n",
    "    Returns:\n",
    "        Chroma vector store\n",
    "    \"\"\"\n",
    "    # Initialize the embedding model\n",
    "    embeddings = get_local_embeddings()\n",
    "    \n",
    "    # Create and persist the vector store\n",
    "    vectordb = Chroma.from_documents(\n",
    "        documents=documents,\n",
    "        embedding=embeddings,\n",
    "        persist_directory=persist_directory,\n",
    "\n",
    "    )\n",
    "    \n",
    "    vectordb.persist()\n",
    "    print(f\"Vector database created with {len(documents)} chunks and saved to {persist_directory}\")\n",
    "    \n",
    "    return vectordb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f0c1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_db = create_vector_db(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "X6jZ743wLayQ",
   "metadata": {
    "id": "X6jZ743wLayQ"
   },
   "source": [
    "## 3.2 Relevant Document Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xj5pH-FATv6S",
   "metadata": {
    "id": "xj5pH-FATv6S"
   },
   "source": [
    "Feel free to check and improve your retrieval performance as it affect the generation results significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6a647547",
   "metadata": {
    "id": "6a647547"
   },
   "outputs": [],
   "source": [
    "def retrieve_documents(query, vectordb, k=1):\n",
    "    \"\"\"\n",
    "    Retrieve relevant documents from the vector database based on the query.\n",
    "    \n",
    "    Args:\n",
    "        query: User query string\n",
    "        vectordb: Vector database to search\n",
    "        k: Number of documents to retrieve\n",
    "    \n",
    "    Returns:\n",
    "        List of retrieved documents\n",
    "    \"\"\"\n",
    "    retriever = vectordb.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": k})\n",
    "    docs = retriever.get_relevant_documents(query)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kYJCpsgHLoc-",
   "metadata": {
    "id": "kYJCpsgHLoc-"
   },
   "source": [
    "## 3.3 Response Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-6xIzmDTn3I8",
   "metadata": {
    "id": "-6xIzmDTn3I8"
   },
   "outputs": [],
   "source": [
    "from src.prompts import generate_answer, load_prompts\n",
    "\n",
    "query = \"What company is located in 29010 Commerce Center Dr., Valencia, 91355, California, US?\"\n",
    "retrieved_docs = retrieve_documents(query, vector_db)\n",
    "prompts = load_prompts()\n",
    "prompt_template = prompts[\"rag_default\"]\n",
    "response = generate_answer(query, retrieved_texts=retrieved_docs, prompt_template=prompt_template, model=\"gpt-4o\")\n",
    "\n",
    "print(\"Query:\", query)\n",
    "print(\"Retrieved Documents:\", [\"ABC Corporation is located at 29010 Commerce Center Dr., Valencia, 91355, California, US.\"])\n",
    "print(\"Generated Answer:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RK8CQELywSf1",
   "metadata": {
    "id": "RK8CQELywSf1"
   },
   "source": [
    "# 4. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfd94a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Evaluation\n",
    "from src.evaluate import evaluate_rag_system, save_evaluation_results\n",
    "\n",
    "# Set evaluation parameters\n",
    "benchmark_file = \"benchmark.json\"  # Path to your benchmark file\n",
    "k = 3  # Number of documents to retrieve for each query\n",
    "prompt_template_name = \"rag_default\"  # Prompt template to use\n",
    "model = \"gpt-4o-mini\"  # Model to use for generation\n",
    "\n",
    "# Run evaluation\n",
    "print(\"Starting evaluation...\")\n",
    "evaluation_results = evaluate_rag_system(\n",
    "    benchmark_file=benchmark_file,\n",
    "    k=k,\n",
    "    prompt_template_name=prompt_template_name,\n",
    "    model=model\n",
    ")\n",
    "\n",
    "# Print summary metrics\n",
    "print(\"\\nEvaluation Results:\")\n",
    "for metric, value in evaluation_results[\"metrics\"].items():\n",
    "    print(\n",
    "        f\"{metric}: {value:.2f}%\"\n",
    "        if \"percentage\" in metric\n",
    "        else f\"{metric}: {value}\"\n",
    "    )\n",
    "\n",
    "# Save results\n",
    "save_evaluation_results(evaluation_results, \"evaluation_results.json\")\n",
    "\n",
    "# Display some example results\n",
    "print(\"\\nExample Results:\")\n",
    "for i, result in enumerate(evaluation_results[\"results\"][:3]):  # Show first 3 examples\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Question: {result['question']}\")\n",
    "    print(f\"Reference: {result['reference_answer']}\")\n",
    "    print(f\"Prediction: {result['predicted_answer']}\")\n",
    "    print(f\"Exact Match: {'Yes' if result['exact_match'] == 1.0 else 'No'}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
